보호 기법 중 유효-무효 비트에 대한 이전 논의는 주로 페이징 시스템에서의 메모리 보호와 요구 페이징을 다루었습니다. 이제 더 넓은 맥락인 메모리 관리에서 **NUMA(Non-Uniform Memory Access) 시스템의 메모리 관리**에 대해 출처의 정보를 활용하여 자세히 논의하겠습니다.

---

**NUMA 시스템과 메모리 관리**

NUMA 시스템은 현대 컴퓨터 시스템의 메모리 관리에서 중요한 고려 사항입니다. 이 아키텍처는 성능 최적화를 위해 특별한 접근 방식을 필요로 합니다.

1.  **NUMA 시스템의 정의 및 특징**
    *   NUMA 시스템은 **여러 개의 CPU로 구성**되며, 각 CPU는 자체적인 로컬 메모리(local memory)를 가집니다.
    *   CPU들은 공유 시스템 인터커넥트(shared system interconnect)를 통해 연결되어 **하나의 물리적 주소 공간을 공유**하지만, **CPU가 자신의 로컬 메모리에 접근하는 속도가 다른 CPU의 로컬 메모리에 접근하는 속도보다 빠릅니다**. 즉, 모든 주 메모리가 균등하게 접근되는 것이 아닙니다.
    *   이러한 성능 차이는 CPU와 메모리가 시스템에서 상호 연결되는 방식 때문에 발생합니다.
    *   NUMA 시스템은 모든 주 메모리 접근이 균등하게 처리되는 시스템보다 느릴 수 있지만, 더 많은 CPU를 수용하여 더 높은 수준의 처리량(throughput)과 병렬성(parallelism)을 달성할 수 있습니다.

2.  **NUMA 환경에서의 메모리 관리 과제**
    *   NUMA 시스템에서는 **페이지 프레임이 어느 위치에 저장되는지를 관리하는 것이 성능에 중대한 영향**을 미칩니다.
    *   메모리를 균일하게 취급하면 CPU는 메모리 접근을 위해 훨씬 더 오래 기다려야 할 수 있습니다.
    *   프로세스의 스레드 수가 많아 여러 시스템 보드에 스케줄링될 경우, 메모리 할당이 복잡해집니다.
    *   **프로세서 선호도(processor affinity)**의 이점(스레드가 동일한 프로세서에서 실행될 때 해당 프로세서의 캐시 메모리에 데이터가 존재할 수 있는 이점)과 **로드 밸런싱(load balancing)**의 목표(스레드를 이동시켜 프로세서 간 부하를 균등화하는 것) 사이에 **자연적인 긴장 관계**가 있습니다. 스레드를 이동시키면 캐시 내용 무효화나 NUMA 시스템에서의 더 긴 메모리 접근 시간으로 인해 성능 저하가 발생할 수 있습니다.

3.  **운영 체제의 NUMA 메모리 관리 전략**
    *   운영 체제는 이러한 NUMA 패널티(CPU가 원격 메모리에 접근할 때 발생하는 지연 시간)를 **신중한 CPU 스케줄링 및 메모리 관리**를 통해 최소화할 수 있습니다.
    *   **NUMA를 고려한 가상 메모리 시스템**은 프로세스가 페이지 폴트(page fault)를 발생시킬 때, 해당 프로세스가 실행 중인 CPU에 **가능한 한 가장 가까운 프레임**을 할당합니다. "가깝다"는 정의는 일반적으로 "최소 대기 시간"을 의미하며, 이는 일반적으로 CPU와 동일한 시스템 보드에 있음을 뜻합니다.
    *   이를 위해 스케줄러는 각 프로세스가 마지막으로 실행된 CPU를 추적합니다. 스케줄러가 각 프로세스를 이전 CPU에 스케줄링하고, 가상 메모리 시스템이 프로세스에 대한 프레임을 스케줄링될 CPU에 가깝게 할당하려고 시도하면, **캐시 적중률(cache hit rates)이 향상되고 메모리 접근 시간(memory access times)이 감소**합니다.

4.  **주요 운영 체제에서의 구현 예시**
    *   **Linux (CFS 스케줄러):**
        *   Linux는 **스케줄링 도메인(scheduling domains)**의 계층적 시스템을 정의하여 NUMA를 고려합니다. 스케줄링 도메인은 서로 균형을 이룰 수 있는 CPU 코어의 집합입니다.
        *   CFS(Completely Fair Scheduler)는 스레드가 다른 도메인으로 마이그레이션하여 메모리 접근 패널티를 유발하는 것을 허용하지 않습니다.
        *   각 NUMA 노드마다 별도의 **자유 프레임 목록(free-frame list)**을 유지하여 스레드가 실행 중인 노드에서 메모리를 할당받도록 보장합니다.
        *   CFS의 일반적인 전략은 계층 구조의 가장 낮은 수준부터 도메인 내에서 부하의 균형을 맞추는 것입니다. 시스템 전체가 바쁘면, CFS는 NUMA 시스템의 메모리 대기 시간 패널티를 피하기 위해 각 코어의 로컬 도메인 이상으로 부하 균형을 맞추지 않으려 합니다.
    *   **Solaris:**
        *   Solaris는 커널에서 **lgroups("locality groups")**를 생성하여 NUMA 문제를 해결합니다.
        *   각 lgroup은 CPU와 메모리를 함께 묶으며, 해당 그룹의 CPU는 정의된 대기 시간 간격 내에서 그룹 내 모든 메모리에 접근할 수 있습니다.
        *   Linux와 유사하게, lgroup 간의 대기 시간 양에 따라 계층 구조가 존재합니다.
        *   Solaris는 프로세스의 모든 스레드를 한 lgroup 내에 스케줄링하고 메모리를 할당하려고 시도합니다. 이것이 불가능하면, 필요한 나머지 리소스에 대해 가까운 lgroup을 선택합니다. 이는 전반적인 메모리 대기 시간을 최소화하고 CPU 캐시 적중률을 최대화합니다.

**메모리 관리의 큰 맥락에서 시사점**
유효-무효 비트와 같은 페이지 테이블의 보호 메커니즘은 프로세스의 논리적 주소 공간을 정의하고, 현재 페이지가 물리 메모리에 있는지 여부를 나타내는 데 사용됩니다. NUMA 시스템에서는 이러한 기본 페이징 메커니즘 위에 **성능을 최적화하기 위한 추가적인 복잡성**이 더해집니다.

페이지 폴트가 발생했을 때 (즉, 유효-무효 비트가 '무효'로 표시되어 페이지가 물리 메모리에 없음을 나타낼 때), NUMA 시스템은 단순히 페이지를 가져오는 것을 넘어 **어느 물리적 프레임에 페이지를 배치할 것인가**를 결정하는 데 NUMA 특성을 고려해야 합니다. 이 결정은 프로세서와 해당 프로세서의 캐시 사이의 거리를 최소화하여 이후의 메모리 접근 시간을 줄이는 것을 목표로 합니다.

또한, NUMA 시스템에서의 메모리 관리는 **스케줄링 알고리즘**과 긴밀하게 연결됩니다. CPU 스케줄러가 스레드를 어떤 코어에 배치할지 결정하는 것은 해당 스레드가 어떤 메모리 영역에 더 빠르게 접근할 수 있는지에 직접적인 영향을 미치기 때문입니다. 이는 운영 체제가 단순히 메모리를 할당하고 해제하는 것을 넘어, 시스템의 전체적인 성능을 최적화하기 위해 **하드웨어 아키텍처의 특성을 깊이 이해하고 통합적인 전략을 수립**해야 함을 보여줍니다.